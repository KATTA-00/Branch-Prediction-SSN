
======================================================================
SNN Sequential PC-Only Model Training Started: 2026-01-11 03:58:34
Log file: outputs/snn_sequential_pc_20260111_035834.log
======================================================================


ðŸ“Š Sequential PC-Only SNN Architecture:
==================================================
   Input: 32 neurons (PC bits only)
   Hidden: 16 LIF neurons
   Output: 2 LIF neurons
   Parameters: 562
   Memory: Maintains neuron state across predictions
âœ“ Sequential training functions defined
   These functions maintain neuron state across batches

======================================================================
          TRAINING SEQUENTIAL PC-ONLY SNN MODEL
======================================================================
Input features: PC bits only (32 neurons)
Sequence length: 50 batches
State maintenance: ENABLED (neuron memory across predictions)
----------------------------------------------------------------------
Epoch [ 1/5] | Train Loss: 0.4062 | Train Acc: 72.30% | Test Loss: 0.6238 | Test Acc: 62.50%
Epoch [ 5/5] | Train Loss: 0.3191 | Train Acc: 81.04% | Test Loss: 1.1934 | Test Acc: 66.55%
----------------------------------------------------------------------
âœ“ Sequential PC-Only Model Training Complete!
   Best Test Accuracy: 66.55%
   Final Test Accuracy: 66.55%

======================================================================
        PC-ONLY: INDEPENDENT vs SEQUENTIAL PROCESSING
======================================================================

ðŸ“Š PC-Only Model Comparison:
----------------------------------------------------------------------
Approach                       Architecture              Test Accuracy  
----------------------------------------------------------------------
Independent Processing         Standard SNN               65.64%
Sequential Processing          Sequential SNN             66.55%
----------------------------------------------------------------------

ðŸŽ¯ Impact of SNN Memory:
   Independent (no memory): 65.64%
   Sequential (with memory): 66.55%
   Improvement: +0.92% (0.92% better)

âœ“ Sequential processing improves accuracy!
  SNN memory helps learn temporal patterns in PC-based branches

ðŸ’¡ Key Insight:
   Sequential processing leverages SNN's inherent memory
   Neuron states carry information between predictions
   This is unique to SNNs - ANNs would need RNN/LSTM layers

======================================================================
âœ“ Comparison saved to 'results/snn_sequential_vs_independent_pc.png'
âœ“ Sequential PC-Only model saved to 'models/snn_sequential_pc_only.pth'

======================================================================
               SEQUENTIAL SNN - FINAL SUMMARY
======================================================================

ðŸ“Š All PC-Only Models Tested:
----------------------------------------------------------------------
1. Independent SNN (PC):        65.64%
2. Sequential SNN (PC):         66.55%
   Difference:                   +0.92%

ðŸ§  What We Learned:
   âœ“ Sequential data handling implemented (no shuffling)
   âœ“ Sequential SNN leverages neuron memory
   âœ“ State maintained across {sequence_length} batches
   âœ“ Temporal patterns in branch execution captured

ðŸŽ¯ SNN Advantages Demonstrated:
   â€¢ Inherent memory through membrane potentials
   â€¢ No need for explicit RNN/LSTM layers
   â€¢ Biologically plausible temporal processing
   â€¢ Event-driven computation potential

======================================================================

======================================================================
Training Session Ended: 2026-01-11 04:01:55
======================================================================
