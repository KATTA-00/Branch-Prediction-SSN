
======================================================================
SNN Branch History-Only Model Training Started: 2026-01-11 06:35:28
Log file: outputs/snn_bh_only_20260111_063528.log
======================================================================

Branch History-Only Dataset:
  Training shape: (255362, 32)
  Test shape: (63841, 32)
  Training batches: 1996
  Test batches: 499

üìä Branch History-Only SNN Architecture:
==================================================
   Input: 32 neurons (Branch History bits only)
   Hidden: 16 LIF neurons
   Output: 2 LIF neurons
   Parameters: 562

======================================================================
             TRAINING BRANCH HISTORY-ONLY SNN MODEL
======================================================================
Input features: Branch History bits only (32 neurons)
----------------------------------------------------------------------
Epoch [ 1/5] | Train Loss: 0.0687 | Train Acc: 97.82% | Test Loss: 0.0012 | Test Acc: 100.00%
Epoch [ 5/5] | Train Loss: 0.0001 | Train Acc: 99.99% | Test Loss: 0.0000 | Test Acc: 100.00%
----------------------------------------------------------------------
‚úì Branch History-Only Model Training Complete!
   Best Test Accuracy: 100.00%
   Final Test Accuracy: 100.00%
data/branch_data_processed_01.csv

======================================================================
               FEATURE ABLATION STUDY - RESULTS
======================================================================

üìä Model Comparison:
----------------------------------------------------------------------
Model                          Input Features       Test Accuracy  
----------------------------------------------------------------------
Full Model (PC + BH)           64 neurons           100.00%
PC-Only Model                  32 neurons            84.60%
Branch History-Only Model      32 neurons           100.00%
----------------------------------------------------------------------

üîç Feature Importance Analysis:
   Full Model Accuracy: 100.00%
   PC Contribution: 84.60%
   Branch History Contribution: 100.00%
   Synergy (Full - Max(PC, BH)): 0.00%

üí° Branch History bits are more predictive (100.00% vs 84.60%)

======================================================================
‚úì Feature ablation comparison saved to 'results/snn_feature_ablation_comparison.png'
‚úì Sequential SNN model defined
   This model can maintain neuron state across predictions
   Useful for learning temporal patterns in branch execution
======================================================================
            SEQUENTIAL vs INDEPENDENT PROCESSING
======================================================================

üîÑ Current Approach (Independent):
   ‚Ä¢ Each branch prediction resets neuron state
   ‚Ä¢ No memory of previous branches
   ‚Ä¢ Branch history in input is the only temporal info
   ‚Ä¢ Treats branches as i.i.d. samples

üß† Sequential Approach (with SNN Memory):
   ‚Ä¢ Neuron states carry over between predictions
   ‚Ä¢ Network 'remembers' recent branch patterns
   ‚Ä¢ Leverages SNN's inherent temporal dynamics
   ‚Ä¢ More realistic for actual CPU execution

üí° Why This Matters for Branch Prediction:
   ‚Ä¢ Branches follow execution flow patterns
   ‚Ä¢ Loop branches repeat regularly
   ‚Ä¢ Function call patterns are predictable
   ‚Ä¢ Context from recent branches improves accuracy

üìä Dataset Structure:
   Total samples: 319,203
   These are sequential branch decisions from CPU execution
   Order matters - shuffling destroys temporal patterns!

‚úì Fixed: Now using sequential split (80/20)
‚úì Fixed: Disabled shuffling in DataLoaders
‚úì Next: Train sequential model to leverage SNN memory

======================================================================
