
======================================================================
SNN Sequential PC-Only Model Training Started: 2026-01-11 05:44:52
Log file: outputs/snn_sequential_pc_20260111_054452.log
======================================================================


ðŸ“Š Sequential PC-Only SNN Architecture:
==================================================
   Input: 32 neurons (PC bits only)
   Hidden: 16 LIF neurons
   Output: 2 LIF neurons
   Parameters: 562
   Memory: Maintains neuron state across predictions
âœ“ Sequential training functions defined
   These functions maintain neuron state across batches

======================================================================
          TRAINING SEQUENTIAL PC-ONLY SNN MODEL
======================================================================
Input features: PC bits only (32 neurons)
Sequence length: 50 batches
State maintenance: ENABLED (neuron memory across predictions)
----------------------------------------------------------------------
Epoch [ 1/5] | Train Loss: 0.3010 | Train Acc: 82.18% | Test Loss: 0.8640 | Test Acc: 77.07%
Epoch [ 5/5] | Train Loss: 0.2738 | Train Acc: 87.75% | Test Loss: 1.6127 | Test Acc: 78.44%
----------------------------------------------------------------------
âœ“ Sequential PC-Only Model Training Complete!
   Best Test Accuracy: 79.18%
   Final Test Accuracy: 78.44%
data/branch_data_processed_01.csv

======================================================================
        PC-ONLY: INDEPENDENT vs SEQUENTIAL PROCESSING
======================================================================

ðŸ“Š PC-Only Model Comparison:
----------------------------------------------------------------------
Approach                       Architecture              Test Accuracy  
----------------------------------------------------------------------
Independent Processing         Standard SNN               84.98%
Sequential Processing          Sequential SNN             78.44%
----------------------------------------------------------------------

ðŸŽ¯ Impact of SNN Memory:
   Independent (no memory): 84.98%
   Sequential (with memory): 78.44%
   Improvement: -6.54% (6.54% worse)

âš  Sequential processing decreased accuracy
  Possible reasons: overfitting, sequence length not optimal

ðŸ’¡ Key Insight:
   Sequential processing leverages SNN's inherent memory
   Neuron states carry information between predictions
   This is unique to SNNs - ANNs would need RNN/LSTM layers

======================================================================
âœ“ Comparison saved to 'results/snn_sequential_vs_independent_pc.png'
âœ“ Sequential PC-Only model saved to 'models/snn_sequential_pc_only.pth'

======================================================================
               SEQUENTIAL SNN - FINAL SUMMARY
======================================================================

ðŸ“Š All PC-Only Models Tested:
----------------------------------------------------------------------
1. Independent SNN (PC):        84.98%
2. Sequential SNN (PC):         78.44%
   Difference:                   -6.54%

ðŸ§  What We Learned:
   âœ“ Sequential data handling implemented (no shuffling)
   âœ“ Sequential SNN leverages neuron memory
   âœ“ State maintained across {sequence_length} batches
   âœ“ Temporal patterns in branch execution captured

ðŸŽ¯ SNN Advantages Demonstrated:
   â€¢ Inherent memory through membrane potentials
   â€¢ No need for explicit RNN/LSTM layers
   â€¢ Biologically plausible temporal processing
   â€¢ Event-driven computation potential

======================================================================

======================================================================
Training Session Ended: 2026-01-11 05:47:44
======================================================================
