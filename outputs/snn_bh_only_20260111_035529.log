
======================================================================
SNN Branch History-Only Model Training Started: 2026-01-11 03:55:29
Log file: outputs/snn_bh_only_20260111_035529.log
======================================================================

Branch History-Only Dataset:
  Training shape: (280626, 32)
  Test shape: (70157, 32)
  Training batches: 2193
  Test batches: 549

üìä Branch History-Only SNN Architecture:
==================================================
   Input: 32 neurons (Branch History bits only)
   Hidden: 16 LIF neurons
   Output: 2 LIF neurons
   Parameters: 562

======================================================================
             TRAINING BRANCH HISTORY-ONLY SNN MODEL
======================================================================
Input features: Branch History bits only (32 neurons)
----------------------------------------------------------------------
Epoch [ 1/5] | Train Loss: 0.1474 | Train Acc: 83.79% | Test Loss: 0.0004 | Test Acc: 100.00%
Epoch [ 5/5] | Train Loss: 0.0001 | Train Acc: 99.99% | Test Loss: 0.0000 | Test Acc: 100.00%
----------------------------------------------------------------------
‚úì Branch History-Only Model Training Complete!
   Best Test Accuracy: 100.00%
   Final Test Accuracy: 100.00%

======================================================================
               FEATURE ABLATION STUDY - RESULTS
======================================================================

üìä Model Comparison:
----------------------------------------------------------------------
Model                          Input Features       Test Accuracy  
----------------------------------------------------------------------
Full Model (PC + BH)           64 neurons           100.00%
PC-Only Model                  32 neurons            65.64%
Branch History-Only Model      32 neurons           100.00%
----------------------------------------------------------------------

üîç Feature Importance Analysis:
   Full Model Accuracy: 100.00%
   PC Contribution: 65.64%
   Branch History Contribution: 100.00%
   Synergy (Full - Max(PC, BH)): 0.00%

üí° Branch History bits are more predictive (100.00% vs 65.64%)

======================================================================
‚úì Feature ablation comparison saved to 'results/snn_feature_ablation_comparison.png'
‚úì Sequential SNN model defined
   This model can maintain neuron state across predictions
   Useful for learning temporal patterns in branch execution
======================================================================
            SEQUENTIAL vs INDEPENDENT PROCESSING
======================================================================

üîÑ Current Approach (Independent):
   ‚Ä¢ Each branch prediction resets neuron state
   ‚Ä¢ No memory of previous branches
   ‚Ä¢ Branch history in input is the only temporal info
   ‚Ä¢ Treats branches as i.i.d. samples

üß† Sequential Approach (with SNN Memory):
   ‚Ä¢ Neuron states carry over between predictions
   ‚Ä¢ Network 'remembers' recent branch patterns
   ‚Ä¢ Leverages SNN's inherent temporal dynamics
   ‚Ä¢ More realistic for actual CPU execution

üí° Why This Matters for Branch Prediction:
   ‚Ä¢ Branches follow execution flow patterns
   ‚Ä¢ Loop branches repeat regularly
   ‚Ä¢ Function call patterns are predictable
   ‚Ä¢ Context from recent branches improves accuracy

üìä Dataset Structure:
   Total samples: 350,783
   These are sequential branch decisions from CPU execution
   Order matters - shuffling destroys temporal patterns!

‚úì Fixed: Now using sequential split (80/20)
‚úì Fixed: Disabled shuffling in DataLoaders
‚úì Next: Train sequential model to leverage SNN memory

======================================================================
