======================================================================
ANN Training Session Started: 2026-01-11 05:36:03
Log file: outputs/ann_full_model_20260111_053603.log
======================================================================

Loading dataset: data/branch_data_processed_01.csv

‚úì Dataset loaded successfully!
Shape: (319203, 65)
Columns: 65

First few rows:
Features shape: (319203, 64)
Target shape: (319203,)

Target distribution:
  Not Taken (0): 140,179 (43.92%)
  Taken (1): 179,024 (56.08%)
‚úì Sequential split (maintains execution order)
Training set size: 255,362 (first 80%)
Test set size: 63,841 (last 20%)
  y_train: torch.Size([255362])

Tensor shapes:
  X_train: torch.Size([255362, 64])
Batch size: 128
Training batches: 1996
Test batches: 499
Artificial Neural Network Architecture:
==================================================
BranchPredictorANN(
  (fc1): Linear(in_features=64, out_features=16, bias=True)
  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout1): Dropout(p=0.3, inplace=False)
  (fc2): Linear(in_features=16, out_features=2, bias=True)
)

Total parameters: 1,106
Loss function: Cross Entropy Loss
Optimizer: Adam (lr=0.001, weight_decay=1e-5)
Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)
‚úì Training and evaluation functions defined!
======================================================================
                    TRAINING ARTIFICIAL NEURAL NETWORK
======================================================================

Epochs: 5
Batch size: 128

----------------------------------------------------------------------
Epoch [ 1/5] | Train Loss: 0.0568 | Train Acc: 98.26% | Test Loss: 0.0005 | Test Acc: 100.00%
Epoch [ 2/5] | Train Loss: 0.0038 | Train Acc: 99.94% | Test Loss: 0.0000 | Test Acc: 100.00%
Epoch [ 3/5] | Train Loss: 0.0018 | Train Acc: 99.98% | Test Loss: 0.0000 | Test Acc: 100.00%
Epoch [ 4/5] | Train Loss: 0.0018 | Train Acc: 99.96% | Test Loss: 0.0000 | Test Acc: 100.00%
Epoch [ 5/5] | Train Loss: 0.0010 | Train Acc: 99.99% | Test Loss: 0.0000 | Test Acc: 100.00%
----------------------------------------------------------------------

‚úì Training complete!
Best Test Accuracy: 100.00%
data/branch_data_processed_01.csv
‚úì Training history saved to 'results/ann_training_history.png'
Running final inference on test set...

======================================================================
                    INFERENCE RESULTS
======================================================================

üéØ Test Accuracy: 100.00%
üìâ Test Loss: 0.0000

üìä Classification Report:
--------------------------------------------------
              precision    recall  f1-score   support

   Not Taken       1.00      1.00      1.00     29398
       Taken       1.00      1.00      1.00     34443

    accuracy                           1.00     63841
   macro avg       1.00      1.00      1.00     63841
weighted avg       1.00      1.00      1.00     63841

‚úì Confusion matrix saved to 'results/ann_confusion_matrix.png'
‚úì ROC curve saved to 'results/ann_roc_curve.png'
   AUC Score: 1.0000
‚úì Confidence distribution saved to 'results/ann_confidence_distribution.png'
‚úì Model saved to 'models/ann_branch_predictor.pth'
======================================================================
                    ANN vs SNN COMPARISON
======================================================================

üìä Architecture (Both Models):
   Input Layer: 64 neurons
   Hidden Layer: 16 neurons
   Output Layer: 2 neurons

üìä ANN Model:
   Parameters: 1,106
   Test Accuracy: 100.00%
   AUC Score: 1.0000
   Training Time: ~5 epochs

üí° Advantages of ANN:
   ‚úì Faster training and inference
   ‚úì More mature optimization techniques
   ‚úì Standard backpropagation
   ‚úì Easier to debug and tune
   ‚úì Better regularization options

‚ö° Advantages of SNN:
   ‚úì Biologically plausible
   ‚úì Event-driven computation
   ‚úì Potential for energy efficiency
   ‚úì Temporal dynamics modeling
   ‚úì Suitable for neuromorphic hardware

======================================================================
======================================================================
               ARTIFICIAL NEURAL NETWORK - SUMMARY
======================================================================

üìä Network Architecture:
   Input Layer: 64 neurons (32 PC + 32 Branch History bits)
   Hidden Layer: 16 neurons (BatchNorm + ReLU + Dropout)
   Output Layer: 2 neurons (Softmax)
   Total Parameters: 1,106

‚öôÔ∏è Training Configuration:
   Epochs: 5
   Batch Size: 128
   Learning Rate: 0.001 (adaptive)
   Optimizer: Adam with weight decay
   Dropout: 0.3

üìà Dataset:
   Training samples: 255,362
   Test samples: 63,841

üéØ Final Results:
   Training Accuracy: 99.99%
   Test Accuracy: 100.00%
   Best Test Accuracy: 100.00%
   AUC Score: 1.0000

üíæ Saved Files:
   Model: models/ann_branch_predictor.pth
   Training History: results/ann_training_history.png
   Confusion Matrix: results/ann_confusion_matrix.png
   ROC Curve: results/ann_roc_curve.png
   Confidence Distribution: results/ann_confidence_distribution.png

======================================================================
‚úì Implementation complete! ANN model trained successfully.
======================================================================
