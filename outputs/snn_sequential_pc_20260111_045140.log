
======================================================================
SNN Sequential PC-Only Model Training Started: 2026-01-11 04:51:40
Log file: outputs/snn_sequential_pc_20260111_045140.log
======================================================================


ðŸ“Š Sequential PC-Only SNN Architecture:
==================================================
   Input: 32 neurons (PC bits only)
   Hidden: 16 LIF neurons
   Output: 2 LIF neurons
   Parameters: 562
   Memory: Maintains neuron state across predictions
âœ“ Sequential training functions defined
   These functions maintain neuron state across batches

======================================================================
          TRAINING SEQUENTIAL PC-ONLY SNN MODEL
======================================================================
Input features: PC bits only (32 neurons)
Sequence length: 500 batches
State maintenance: ENABLED (neuron memory across predictions)
----------------------------------------------------------------------
Epoch [ 1/5] | Train Loss: 0.4361 | Train Acc: 74.03% | Test Loss: 0.8462 | Test Acc: 64.21%
Epoch [ 5/5] | Train Loss: 0.5015 | Train Acc: 64.59% | Test Loss: 0.9216 | Test Acc: 57.12%
----------------------------------------------------------------------
âœ“ Sequential PC-Only Model Training Complete!
   Best Test Accuracy: 64.21%
   Final Test Accuracy: 57.12%

======================================================================
        PC-ONLY: INDEPENDENT vs SEQUENTIAL PROCESSING
======================================================================

ðŸ“Š PC-Only Model Comparison:
----------------------------------------------------------------------
Approach                       Architecture              Test Accuracy  
----------------------------------------------------------------------
Independent Processing         Standard SNN               66.41%
Sequential Processing          Sequential SNN             57.12%
----------------------------------------------------------------------

ðŸŽ¯ Impact of SNN Memory:
   Independent (no memory): 66.41%
   Sequential (with memory): 57.12%
   Improvement: -9.29% (9.29% worse)

âš  Sequential processing decreased accuracy
  Possible reasons: overfitting, sequence length not optimal

ðŸ’¡ Key Insight:
   Sequential processing leverages SNN's inherent memory
   Neuron states carry information between predictions
   This is unique to SNNs - ANNs would need RNN/LSTM layers

======================================================================
âœ“ Comparison saved to 'results/snn_sequential_vs_independent_pc.png'
âœ“ Sequential PC-Only model saved to 'models/snn_sequential_pc_only.pth'

======================================================================
               SEQUENTIAL SNN - FINAL SUMMARY
======================================================================

ðŸ“Š All PC-Only Models Tested:
----------------------------------------------------------------------
1. Independent SNN (PC):        66.41%
2. Sequential SNN (PC):         57.12%
   Difference:                   -9.29%

ðŸ§  What We Learned:
   âœ“ Sequential data handling implemented (no shuffling)
   âœ“ Sequential SNN leverages neuron memory
   âœ“ State maintained across {sequence_length} batches
   âœ“ Temporal patterns in branch execution captured

ðŸŽ¯ SNN Advantages Demonstrated:
   â€¢ Inherent memory through membrane potentials
   â€¢ No need for explicit RNN/LSTM layers
   â€¢ Biologically plausible temporal processing
   â€¢ Event-driven computation potential

======================================================================

======================================================================
Training Session Ended: 2026-01-11 04:55:09
======================================================================
